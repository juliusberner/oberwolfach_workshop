{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\varphi_j(x) = \\cos(\\pi jx)$ (modulo factors of $\\pi$ :) ).\n",
    "Given data points $(x_i,y_i)_{i=1}^m \\subset (a,b)\\times \\mathbb{R}$ generated as follows: Let $X\\sim \\mathcal{U}[0,1]$ and let $Y = \\hat f(X)+\\eta$ where $\\hat f$ is sparse in $(\\varphi_j)_{j}$, for example $\\hat f(x) = \\cos(\\pi x ) + \\cos(5 \\pi x)$ we want to estimate $\\hat f$. In other words, we want to solve the statistical learning problem associated to $(X,Y)$. We will consider the hypothesis classes $\\mathcal{H}_N:=\\mathrm{span}\\{\\varphi_j:\\ j = 1,\\dots , N\\}$.\n",
    "\n",
    "In empirical risk minimization (ERM) we are trying to fit a function $f:(a,b)\\to \\mathbb{R}$, $f(x)=\\sum_{j=1}^N c_j \\varphi_j(x)$ such that $\\sum_{i=1}^m(f(x_i)- y_i)^2$ is minimized over $\\mathcal{H}_N$. As is predicted by statistical learning theory, the generalization error will grow as the complexity of $\\mathcal{H}_N$ grows. In other words, the number of necessary samples has to grow with $N$. \n",
    "\n",
    "The aim of this notebook is to show that one can do much better if one uses an algorithm different from ERM. This algorithm is called LASSO. Similar to ERM in LASSO we solve an optimization problem but this time we minimize the *$\\ell^1$-regularized* least squares error $\\sum_{i=1}^m(\\sum_{j=1}^N c_j \\varphi_j(x_i)- y_i)^2 + \\alpha \\sum_{j=1}^N|c_j|$ for some regulatization parameter $\\alpha$. We will see that this algorithm significantly outperforms what we might expect from statistical learning theory. A theoretical explanation for this behaviour is provided by the field of Compressive Sensing.\n",
    "\n",
    "\n",
    "In the last part we solve another optimization problem; this time we minimize the *$\\ell^2$-regularized* least squares error $\\sum_{i=1}^m(\\sum_{j=1}^N c_j \\varphi_j(x_i)- y_i)^2 + \\alpha \\sum_{j=1}^N|c_j|^2$ for some regulatization parameter $\\alpha$. This algorithm is sometimes called *Elastic Net Regularization* We will see that this algorithm again behaves a bit more similarly to what would be expected from statistical learning. It is also not hard to see that SGD  would converge to an $\\ell^2$-regularized solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write a function that generates our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(m=100,a=0,b=20,noiselevel = 1, noisetype = \"normal\", \n",
    "            regression_function = lambda x : np.cos(5*np.pi*x/20) + np.cos(x*np.pi/20)):\n",
    "    t = np.random.uniform(a,b,m) \n",
    "    t = np.sort(t)              \n",
    "    sig = regression_function(t)      \n",
    "    if noisetype == \"normal\":\n",
    "        noise = np.random.normal(np.zeros(np.shape(sig)))\n",
    "    if noisetype == \"cauchy\":\n",
    "        noise = np.random.standard_cauchy(np.shape(sig))\n",
    "    signoise = sig+noiselevel*noise \n",
    "    signoise.reshape(m,1) \n",
    "    return t, signoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write functions that solve the ERM and LASSO problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least squares regression code\n",
    "def LeastSquares(Basisfunct , X , Y ):\n",
    "    A = Basisfunct(X)\n",
    "    m = np.shape(A)[0]\n",
    "    G = np.zeros((m,m))\n",
    "    b = np.zeros(m)\n",
    "    #generate Gram matrix\n",
    "    for i in range(m):\n",
    "        b[i]=np.sum(np.multiply(A[i],Y))\n",
    "        for j in range(m):\n",
    "            G[i,j]=np.sum(np.multiply(A[i],A[j]))\n",
    "    #find optimal parameters\n",
    "    params = np.linalg.solve(G,b)\n",
    "    #print condition number of the linear system\n",
    "    cond = np.linalg.cond(G)\n",
    "    #print(\"condition number = {}\".format(cond))\n",
    "    return  params \n",
    "\n",
    "def Lasso(Basisfunct, X , Y , alpha = 0.1):\n",
    "    A = Basisfunct(X)\n",
    "    m = np.shape(A)[0]\n",
    "    G = np.zeros((m,m))\n",
    "    b = np.zeros(m)\n",
    "    #generate Gram matrix\n",
    "    for i in range(m):\n",
    "        b[i]=np.sum(np.multiply(A[i],Y))\n",
    "        for j in range(m):\n",
    "            G[i,j]=np.sum(np.multiply(A[i],A[j]))\n",
    "    #find optimal parameters\n",
    "    #clf = linear_model.Lasso(alpha=alpha)\n",
    "    clf = linear_model.ElasticNet(alpha=alpha , l1_ratio = 1 , max_iter = 1000)\n",
    "    clf.fit(G,b)\n",
    "    params = clf.coef_\n",
    "    #print condition number of the linear system\n",
    "    cond = np.linalg.cond(G)\n",
    "    #print(\"condition number = {}\".format(cond))\n",
    "    return  params \n",
    "\n",
    "def L2Regularization(Basisfunct, X , Y , alpha = 1.0):\n",
    "    A = Basisfunct(X)\n",
    "    m = np.shape(A)[0]\n",
    "    G = np.zeros((m,m))\n",
    "    b = np.zeros(m)\n",
    "    #generate Gram matrix\n",
    "    for i in range(m):\n",
    "        b[i]=np.sum(np.multiply(A[i],Y))\n",
    "        for j in range(m):\n",
    "            G[i,j]=np.sum(np.multiply(A[i],A[j]))\n",
    "    #find optimal parameters\n",
    "    clf = linear_model.ElasticNet(alpha=alpha , l1_ratio = 0 , max_iter = 5000)\n",
    "    clf.fit(G,b)\n",
    "    params = clf.coef_\n",
    "    #print condition number of the linear system\n",
    "    cond = np.linalg.cond(G)\n",
    "    #print(\"condition number = {}\".format(cond))\n",
    "    return  params \n",
    "\n",
    "def Evaluate(Basisfunct , X , params):\n",
    "    #evaluate fit on validation set\n",
    "    fit = np.zeros(np.shape(X))\n",
    "    C = Basisfunct(X)\n",
    "    for i in range(len(params)):\n",
    "        fit = fit + params[i]*C[i]\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 30\n",
    "nval = 1000\n",
    "noise = 0.4\n",
    "Phi = lambda x: [np.cos(np.pi*i*x/20) for i in range(100)]\n",
    "regcoeffs = np.zeros(100)\n",
    "#regcoeffs[1]=1\n",
    "#regcoeffs[6]=1\n",
    "for i in range(1,10):\n",
    "    regcoeffs[-i]=i**(-1)\n",
    "#regression_function = lambda x : np.cos(5*np.pi*x/20) + np.cos(x*np.pi/20)\n",
    "regression_function = lambda x : Evaluate(Phi , x , regcoeffs)\n",
    "X,Y = datagen(ntrain, noiselevel = noise, regression_function = regression_function)\n",
    "Xval , Yval = datagen(nval, noiselevel = noise , regression_function = regression_function)\n",
    "maxdeg = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our regression function is very oscillatory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=np.linspace(0,20,1000)\n",
    "plt.plot(X,Y,'ko',t,regression_function(t),'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the performance of LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainerr = np.zeros([maxdeg-1])\n",
    "valerr = np.zeros([maxdeg-1])\n",
    "for degree in range(1,maxdeg):\n",
    "    #print(\"degree =  {} \".format(degree))\n",
    "    Basisfunct = lambda x: [np.cos(np.pi*i*x/20) for i in range(degree)]\n",
    "    params = Lasso( Basisfunct, X, Y, alpha = 5)\n",
    "    trainerr[degree-1] = 1/np.sqrt(len(X))*np.linalg.norm(Y - Evaluate(Basisfunct, X , params))\n",
    "    valerr[degree-1]=1/np.sqrt(len(Xval))*np.linalg.norm(Yval - Evaluate(Basisfunct, Xval , params))\n",
    "    t=np.linspace(0,20,1000)\n",
    "    plt.plot(X,Y,'ko',t,regression_function(t),'r',t,Evaluate(Basisfunct,t,params),'b')\n",
    "    plt.title(\"degree = {}, samples = {}\".format(degree,ntrain))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainerr, label = \"Training Error\")\n",
    "#plt.plot(valerr, label = \"Validation Error\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(trainerr, label = \"Training Error\")\n",
    "plt.plot(valerr, label = \"Validation Error\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(trainerr, label = \"Training Error\")\n",
    "plt.plot(valerr, label = \"Validation Error\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.title(\"Training and Validation Error for LASSO\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the performance of ERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainerr = np.zeros([maxdeg-1])\n",
    "valerr = np.zeros([maxdeg-1])\n",
    "for degree in range(1,maxdeg):\n",
    "    #print(\"degree =  {} \".format(degree))\n",
    "    Basisfunct = lambda x: [np.cos(np.pi*i*x/20) for i in range(degree)]\n",
    "    params = LeastSquares( Basisfunct, X, Y)\n",
    "    trainerr[degree-1] = 1/np.sqrt(len(X))*np.linalg.norm(Y - Evaluate(Basisfunct, X , params))\n",
    "    valerr[degree-1]=1/np.sqrt(len(Xval))*np.linalg.norm(Yval - Evaluate(Basisfunct, Xval , params))\n",
    "    t=np.linspace(0,20,1000)\n",
    "    plt.plot(X,Y,'ko',t,regression_function(t),'r',t,Evaluate(Basisfunct,t,params),'b')\n",
    "    plt.title(\"degree = {}, samples = {}\".format(degree,ntrain))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainerr, label = \"Training Error\")\n",
    "#plt.plot(valerr, label = \"Validation Error\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(trainerr, label = \"Training Error\")\n",
    "plt.plot(valerr, label = \"Validation Error\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainerr, label = \"Training Error\")\n",
    "plt.plot(valerr, label = \"Validation Error\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.title(\"Training and Validation Error for Emprirical Risk Minimization\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valerr[-10:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets try Elastic Net Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainerr = np.zeros([maxdeg-1])\n",
    "valerr = np.zeros([maxdeg-1])\n",
    "for degree in range(1,maxdeg):\n",
    "    #print(\"degree =  {} \".format(degree))\n",
    "    Basisfunct = lambda x: [np.cos(np.pi*i*x/20) for i in range(degree)]\n",
    "    params = L2Regularization( Basisfunct, X, Y, alpha = 0.1)\n",
    "    trainerr[degree-1] = 1/np.sqrt(len(X))*np.linalg.norm(Y - Evaluate(Basisfunct, X , params))\n",
    "    valerr[degree-1]=1/np.sqrt(len(Xval))*np.linalg.norm(Yval - Evaluate(Basisfunct, Xval , params))\n",
    "    t=np.linspace(0,20,1000)\n",
    "    plt.plot(X,Y,'ko',t,regression_function(t),'r',t,Evaluate(Basisfunct,t,params),'b')\n",
    "    plt.title(\"degree = {}, samples = {}\".format(degree,ntrain))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainerr, label = \"Training Error\")\n",
    "#plt.plot(valerr, label = \"Validation Error\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(trainerr, label = \"Training Error\")\n",
    "plt.plot(valerr, label = \"Validation Error\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainerr, label = \"Training Error\")\n",
    "plt.plot(valerr, label = \"Validation Error\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.title(\"Training and Validation Error for Elastic Net Regularization\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a double descent curve!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
